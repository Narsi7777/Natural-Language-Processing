{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "416886eb-b656-441d-b1bd-f49e6d882d33",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[100], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m data\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m----> 2\u001b[0m n\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menter no of rows in training set\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n\u001b[0;32m      4\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menter word and enter done for end of each row\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1262\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1260\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[0;32m   1263\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[0;32m   1264\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1266\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1267\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1305\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1303\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1304\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "data=[]\n",
    "n=int(input(\"enter no of rows in training set\"))\n",
    "for i in range(n):\n",
    "    x=input(\"enter word and enter done for end of each row\")\n",
    "    r=[]\n",
    "    while x!=\"done\":\n",
    "      r.append(x)\n",
    "      x=input(\"enter word and enter done for end of each row\")\n",
    "    s=input(\"enter sense of this row\")\n",
    "    data.append((r,s))\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "48c95660-6802-4f8b-ae06-62a77b210ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter no of words 4\n",
      "enter word by word in new training set efr\n",
      "enter word by word in new training set rerf\n",
      "enter word by word in new training set erfw\n",
      "enter word by word in new training set dw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['efr', 'rerf', 'erfw', 'dw']\n"
     ]
    }
   ],
   "source": [
    "new_data=[]\n",
    "z=int(input(\"enter no of words\"))\n",
    "for s in range(z):\n",
    "    new_data.append(input(\"enter word by word in new training set\"))\n",
    "print(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3dac6327-ddb4-4789-bae7-7fb5d765ac0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (['bass', 'eat', 'super'], 'fish'),\n",
    "    (['bass', 'lunch', 'excellent'], 'fish'),\n",
    "    (['bass', 'ate', 'like'], 'fish'),\n",
    "    (['bass', 'play', 'music'], 'music'),\n",
    "    (['bass', 'interest', 'play'], 'music')\n",
    "]\n",
    "\n",
    "\n",
    "newData = ['bass', 'super', 'excellent', 'play']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200bf1d7-ec00-4af7-9748-b399dbf2e69d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d412f66e-620b-429e-bb84-5d0c2edb6461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'fish': 3, 'music': 2})\n",
      "{'super', 'bass', 'eat', 'ate', 'play', 'music', 'lunch', 'excellent', 'like', 'interest'}\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "V=set()\n",
    "senses=defaultdict(int)\n",
    "for row in data:\n",
    "    senses[row[1]]=senses[row[1]]+1\n",
    "    for wrd in row[0]:\n",
    "        V.add(wrd)\n",
    "print(senses)\n",
    "print(V)\n",
    "V=len(V)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3dfb4364-58ee-4e4c-930d-78c2ed48f36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'float'>, {'fish': 0.26666666666666666, 'music': 0.2})\n"
     ]
    }
   ],
   "source": [
    "#prior Probababilities\n",
    "n=len(data)\n",
    "prior_prob=defaultdict(float)\n",
    "for sen,cnt in senses.items():\n",
    "    prior_prob[sen]=(cnt+1)/(n+V)\n",
    "print(prior_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e80e48ba-fa1a-44aa-86a9-ded7a56c3d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function <lambda> at 0x000002BE832D3A60>, {'fish': defaultdict(<class 'int'>, {'bass': 3, 'eat': 1, 'super': 1, 'lunch': 1, 'excellent': 1, 'ate': 1, 'like': 1}), 'music': defaultdict(<class 'int'>, {'bass': 2, 'play': 2, 'music': 1, 'interest': 1})})\n"
     ]
    }
   ],
   "source": [
    "#conditinal_probabilities_counts\n",
    "word_counts=defaultdict(lambda:defaultdict(int))\n",
    "for row in data:\n",
    "    for wrd in row[0]:\n",
    "        word_counts[row[1]][wrd]+=1\n",
    "print(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "80707416-1d91-4c3e-94f4-1ce6a09eb25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'float'>, {'fish': -8.808964547588685, 'music': -9.351839934249883})\n",
      "-9999\n",
      "music\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "outputs=defaultdict(float)\n",
    "for sn in senses:\n",
    "    outputs[sn]=math.log(prior_prob[sn])\n",
    "    for wrd in newData:\n",
    "        outputs[sn]+=math.log((word_counts[sn][wrd]+1)/(senses[sn]+V))\n",
    "print(outputs)\n",
    "ans=-9999\n",
    "va=\"\"\n",
    "for key,val in outputs.items():\n",
    "    if val>ans:\n",
    "        val=ans\n",
    "        va=key\n",
    "print(ans)\n",
    "print(va)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e981729-8f56-4dca-b013-052b7932d14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Senses: defaultdict(<class 'int'>, {'furniture': 4, 'position': 2})\n",
      "Vocab Size: 17\n",
      "Prior Probabilities: defaultdict(<class 'float'>, {'furniture': 0.625, 'position': 0.375})\n",
      "Word Counts per Class: {'furniture': defaultdict(<class 'int'>, {'put': 1, 'coat': 1, 'back': 1, 'chair': 4, 'sat': 1, 'down': 1, 'made': 1, 'timber': 1, 'company': 2, 'type': 1, 'different': 1, 'award': 2, 'fun': 1, 'use': 1, 'it': 1}), 'position': defaultdict(<class 'int'>, {'chair': 2, 'institute': 1, 'best': 1, 'award': 1, 'it': 1})}\n",
      "Log-probabilities: defaultdict(<class 'float'>, {'furniture': -8.148283709809164, 'position': -10.273678519889488})\n",
      "Predicted Sense: furniture\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "data = [\n",
    "    (['put', 'coat', 'back', 'chair', 'sat', 'down'], 'furniture'),\n",
    "    (['chair', 'made', 'timber', 'company'], 'furniture'),\n",
    "    (['chair', 'institute', 'best'], 'position'),\n",
    "    (['award', 'it', 'chair'], 'position'),\n",
    "    (['type', 'different', 'chair', 'award', 'fun', 'use'], 'furniture'),\n",
    "    (['award', 'chair', 'it', 'company'], 'furniture')\n",
    "]\n",
    "\n",
    "newData = ['chair', 'award', 'it', 'company']\n",
    "\n",
    "\n",
    "# Step 1: Count senses and build vocabulary\n",
    "V = set()\n",
    "senses = defaultdict(int)\n",
    "for row in data:\n",
    "    senses[row[1]] += 1\n",
    "    for wrd in row[0]:\n",
    "        V.add(wrd.lower())  # normalize to lowercase\n",
    "V = len(V)\n",
    "print(\"Senses:\", senses)\n",
    "print(\"Vocab Size:\", V)\n",
    "\n",
    "# Step 2: Prior probabilities (with Laplace smoothing)\n",
    "n = len(data)\n",
    "prior_prob = defaultdict(float)\n",
    "for sense, cnt in senses.items():\n",
    "    prior_prob[sense] = (cnt + 1) / (n + len(senses))\n",
    "print(\"Prior Probabilities:\", prior_prob)\n",
    "\n",
    "# Step 3: Conditional probabilities counts\n",
    "word_counts = defaultdict(lambda: defaultdict(int))\n",
    "for row in data:\n",
    "    for wrd in row[0]:\n",
    "        word_counts[row[1]][wrd.lower()] += 1\n",
    "print(\"Word Counts per Class:\", dict(word_counts))\n",
    "\n",
    "# Step 4: Naive Bayes classification\n",
    "outputs = defaultdict(float)\n",
    "for sn in senses:\n",
    "    outputs[sn] = math.log(prior_prob[sn])  # initialize log(P(class))\n",
    "    for wrd in newData:\n",
    "        wrd = wrd.lower()\n",
    "        count = word_counts[sn][wrd]\n",
    "        outputs[sn] += math.log((count + 1) / (senses[sn] + V))  # Laplace smoothing\n",
    "\n",
    "print(\"Log-probabilities:\", outputs)\n",
    "predicted = max(outputs, key=outputs.get)\n",
    "print(\"Predicted Sense:\", predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fe27d0ea-6d6a-4beb-8483-722090f94a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the room was filled with bright light.'], ['the sunrise brings natural light every morning.'], [\"the candle's light flickered in the wind.\"], ['she turned on the light in the dark room.'], ['the light from the stars was visible at night.'], ['this bag is very light and easy to carry.'], ['the box felt light compared to the others.'], ['he packed only light clothes for the summer trip.'], ['the chair is made of a light material.'], ['the suitcase was surprisingly light.'], ['the traffic light turned red.'], ['he crossed the street when the light was green.'], ['the yellow light warns drivers to slow down.'], ['the car stopped at the red light.'], ['the pedestrian waited for the signal light to change.'], ['she saw a bright light in the sky.'], ['2 the backpack is very light to carry.'], ['3 the car moved forward when the light turned green.']]\n",
      "Senses: defaultdict(<class 'int'>, {'Brightness': 5, 'Weight': 5, 'Traffic Signal': 5})\n",
      "Vocab Size: 70\n",
      "Prior Probabilities: defaultdict(<class 'float'>, {'Brightness': 0.07058823529411765, 'Weight': 0.07058823529411765, 'Traffic Signal': 0.07058823529411765})\n",
      "Word Counts per Class: {'Brightness': defaultdict(<class 'int'>, {'the': 8, 'room': 2, 'was': 2, 'filled': 1, 'with': 1, 'bright': 1, 'light': 5, '.': 5, 'sunrise': 1, 'brings': 1, 'natural': 1, 'every': 1, 'morning': 1, 'candle': 1, \"'s\": 1, 'flickered': 1, 'in': 2, 'wind': 1, 'she': 1, 'turned': 1, 'on': 1, 'dark': 1, 'from': 1, 'stars': 1, 'visible': 1, 'at': 1, 'night': 1}), 'Weight': defaultdict(<class 'int'>, {'this': 1, 'bag': 1, 'is': 2, 'very': 1, 'light': 5, 'and': 1, 'easy': 1, 'to': 2, 'carry': 1, '.': 5, 'the': 5, 'box': 1, 'felt': 1, 'compared': 1, 'others': 1, 'he': 1, 'packed': 1, 'only': 1, 'clothes': 1, 'for': 1, 'summer': 1, 'trip': 1, 'chair': 1, 'made': 1, 'of': 1, 'a': 1, 'material': 1, 'suitcase': 1, 'was': 1, 'surprisingly': 1}), 'Traffic Signal': defaultdict(<class 'int'>, {'the': 8, 'traffic': 1, 'light': 5, 'turned': 1, 'red': 2, '.': 5, 'he': 1, 'crossed': 1, 'street': 1, 'when': 1, 'was': 1, 'green': 1, 'yellow': 1, 'warns': 1, 'drivers': 1, 'to': 2, 'slow': 1, 'down': 1, 'car': 1, 'stopped': 1, 'at': 1, 'pedestrian': 1, 'waited': 1, 'for': 1, 'signal': 1, 'change': 1})}\n",
      "Log-probabilities: defaultdict(<class 'float'>, {'Brightness': -41.47214576247319, 'Weight': -42.97622315924946, 'Traffic Signal': -39.39270422079335})\n",
      "Predicted Sense: Traffic Signal\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_excel('WSD_light.xlsx')\n",
    "x=df['Sentence'].tolist()\n",
    "y=df['Sense Category'].tolist()\n",
    "# print(x[10],y[10])\n",
    "from nltk.tokenize import word_tokenize\n",
    "x=[[sent.lower()] for sent in x]\n",
    "print(x)\n",
    "x=[word_tokenize(x[i][0]) for i in range(len(x))]\n",
    "\n",
    "data=[]\n",
    "for i in range(15):\n",
    "    data.append((x[i],y[i]))\n",
    "\n",
    "newData=x[17]\n",
    "\n",
    "# Step 1: Count senses and build vocabulary\n",
    "V = set()\n",
    "senses = defaultdict(int)\n",
    "for row in data:\n",
    "    senses[row[1]] += 1\n",
    "    for wrd in row[0]:\n",
    "        V.add(wrd.lower())  # normalize to lowercase\n",
    "V = len(V)\n",
    "print(\"Senses:\", senses)\n",
    "print(\"Vocab Size:\", V)\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Prior probabilities (with Laplace smoothing)\n",
    "n = len(data)\n",
    "prior_prob = defaultdict(float)\n",
    "for sense, cnt in senses.items():\n",
    "    prior_prob[sense] = (cnt + 1) / (n + V)\n",
    "print(\"Prior Probabilities:\", prior_prob)\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Conditional probabilities counts\n",
    "word_counts = defaultdict(lambda: defaultdict(int))\n",
    "for row in data:\n",
    "    for wrd in row[0]:\n",
    "        word_counts[row[1]][wrd.lower()] += 1\n",
    "print(\"Word Counts per Class:\", dict(word_counts))\n",
    "\n",
    "\n",
    "\n",
    "# Step 4: Naive Bayes classification\n",
    "outputs = defaultdict(float)\n",
    "for sn in senses:\n",
    "    outputs[sn] = math.log(prior_prob[sn])  # initialize log(P(class))\n",
    "    for wrd in newData:\n",
    "        wrd = wrd.lower()\n",
    "        count = word_counts[sn][wrd]\n",
    "        outputs[sn] += math.log((count + 1) / (senses[sn] + V))  # Laplace smoothing\n",
    "\n",
    "print(\"Log-probabilities:\", outputs)\n",
    "predicted = max(outputs, key=outputs.get)\n",
    "print(\"Predicted Sense:\", predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e220caf-fede-46ec-9315-4c43dfea0282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian Word Sense Disambiguation for the word 'light'\n",
    "import pandas as pd\n",
    "import string\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import math\n",
    "\n",
    "# Load the Excel file\n",
    "filename = 'WSD_light.xlsx'\n",
    "data = pd.read_excel(filename,nrows=16)\n",
    "test_data = pd.read_excel(filename, skiprows=16)\n",
    "\n",
    "# Preprocessing: Tokenization and Cleaning\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Extracting sentences and senses\n",
    "sentences = data['Sentence'].apply(preprocess)\n",
    "senses = data['Sense Category']\n",
    "\n",
    "# Calculate prior probabilities\n",
    "sense_counts = Counter(senses)\n",
    "total_instances = len(senses)\n",
    "priors = {sense: count / total_instances for sense, count in sense_counts.items()}\n",
    "\n",
    "# Calculate conditional probabilities with Laplace smoothing\n",
    "word_counts = defaultdict(lambda: defaultdict(int))\n",
    "sense_totals = defaultdict(int)\n",
    "vocab = set()\n",
    "\n",
    "for sentence, sense in zip(sentences, senses):\n",
    "    for word in sentence:\n",
    "        word_counts[sense][word] += 1\n",
    "        sense_totals[sense] += 1\n",
    "        vocab.add(word)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Laplace smoothing for conditional probabilities\n",
    "def conditional_prob(word, sense):\n",
    "    return (word_counts[sense][word] + 1) / (sense_totals[sense] + vocab_size)\n",
    "\n",
    "# Bayesian classification of a test sentence\n",
    "def predict_sense(sentence):\n",
    "    tokens = preprocess(sentence)\n",
    "    max_prob = -float('inf')\n",
    "    best_sense = None\n",
    "    for sense in priors:\n",
    "        log_prob = math.log(priors[sense])\n",
    "        for word in tokens:\n",
    "            log_prob += math.log(conditional_prob(word, sense))\n",
    "        if log_prob > max_prob:\n",
    "            max_prob = log_prob\n",
    "            best_sense = sense\n",
    "    return best_sense\n",
    "\n",
    "# Test sentences containing 'light'\n",
    "test_sentences = [\n",
    "    'She saw a bright light in the sky.',\n",
    "    'The backpack is very light to carry.',\n",
    "    'The car moved forward when the light turned green.'\n",
    "]\n",
    "\n",
    "# Predict the sense for each test sentence\n",
    "for sentence in test_sentences:\n",
    "    predicted_sense = predict_sense(sentence)\n",
    "    print(f'Sentence: \"{sentence}\" => Predicted Sense: {predicted_sense}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
